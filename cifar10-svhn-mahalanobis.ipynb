{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e50fb18-2f01-407e-b1b4-e9b9dff81375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /data/home/mukhotij/miniconda/envs/pytorch/lib/python3.8/site-packages (4.5.2.54)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /data/home/mukhotij/miniconda/envs/pytorch/lib/python3.8/site-packages (from opencv-python) (1.21.2)\n"
     ]
    }
   ],
   "source": [
    "import typing\n",
    "import io\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "!pip install opencv-python\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "from models.modeling import VisionTransformer, CONFIGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "930bb5d6-6805-4c73-9ee0-d04b13e27962",
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    img_size = 224\n",
    "    pretrained_dir = \"output_models/cifar10/ViT-L_32\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dataset = \"cifar10\"\n",
    "    local_rank = -1\n",
    "    train_batch_size = 64\n",
    "    eval_batch_size = 64\n",
    "    \n",
    "\n",
    "config_vit_b_16 = CONFIGS[\"ViT-B_16\"]\n",
    "config_vit_b_32 = CONFIGS[\"ViT-B_32\"]\n",
    "config_vit_l_16 = CONFIGS[\"ViT-L_16\"]\n",
    "config_vit_l_32 = CONFIGS[\"ViT-L_32\"]\n",
    "\n",
    "models = []\n",
    "\n",
    "for i in range(1,4):\n",
    "    model = VisionTransformer(config_vit_l_32, args.img_size, zero_head=True, num_classes=10, vis=True)\n",
    "    model.load_state_dict(torch.load(args.pretrained_dir + f'/Run{i}/{args.dataset}_vit_l_32_seed_{i}_checkpoint.bin'))\n",
    "    model.to(args.device)\n",
    "    models.append(model)\n",
    "# model = VisionTransformer(config_vit_b_16, args.img_size, zero_head=True, num_classes=10, vis=True)\n",
    "# model.load_state_dict(torch.load(args.pretrained_dir + f'/Run1/{args.dataset}_vit_b_16_seed_1_checkpoint.bin'))\n",
    "# model.to(args.device)\n",
    "# num_params = count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "14e48f1f-601e-45ad-abea-429d5b3133f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: ./data/train_32x32.mat\n",
      "Using downloaded and verified file: ./data/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "# Load the data loaders (CIFAR-10 test loader and SVHN test loader)\n",
    "from utils.data_utils import get_loader\n",
    "from ipywidgets import IntProgress\n",
    "\n",
    "cifar10_train_loader, cifar10_test_loader = get_loader(args)\n",
    "args.dataset = \"cifar100\"\n",
    "cifar100_train_loader, cifar100_test_loader = get_loader(args)\n",
    "args.dataset = \"svhn\"\n",
    "svhn_train_loader, svhn_test_loader = get_loader(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea21a302-0f07-42b3-8d39-b810a242e6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9902, dtype=torch.float64)\n",
      "tensor(0.0002, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def get_logits_labels(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Utility function to get logits and labels.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    logits = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for data, label in data_loader:\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            logit, _ = model(data)\n",
    "            logits.append(logit)\n",
    "            labels.append(label)\n",
    "    logits = torch.cat(logits, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    return logits, labels\n",
    "\n",
    "def test_classification_net_softmax(softmax_prob, labels):\n",
    "    \"\"\"\n",
    "    This function reports classification accuracy and confusion matrix given softmax vectors and\n",
    "    labels from a model.\n",
    "    \"\"\"\n",
    "    labels_list = []\n",
    "    predictions_list = []\n",
    "    confidence_vals_list = []\n",
    "\n",
    "    confidence_vals, predictions = torch.max(softmax_prob, dim=1)\n",
    "    labels_list.extend(labels.cpu().numpy())\n",
    "    predictions_list.extend(predictions.cpu().numpy())\n",
    "    confidence_vals_list.extend(confidence_vals.cpu().numpy())\n",
    "    accuracy = accuracy_score(labels_list, predictions_list)\n",
    "    return (\n",
    "        confusion_matrix(labels_list, predictions_list),\n",
    "        accuracy,\n",
    "        labels_list,\n",
    "        predictions_list,\n",
    "        confidence_vals_list,\n",
    "    )\n",
    "\n",
    "def test_classification_net_logits(logits, labels):\n",
    "    \"\"\"\n",
    "    This function reports classification accuracy and confusion matrix given logits and labels\n",
    "    from a model.\n",
    "    \"\"\"\n",
    "    softmax_prob = F.softmax(logits, dim=1)\n",
    "    return test_classification_net_softmax(softmax_prob, labels)\n",
    "\n",
    "\n",
    "def test_classification_net(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    This function reports classification accuracy and confusion matrix over a dataset.\n",
    "    \"\"\"\n",
    "    logits, labels = get_logits_labels(model, data_loader, device)\n",
    "    return test_classification_net_logits(logits, labels)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "accs = []\n",
    "for model in models:\n",
    "    _, accuracy, _, _, _ = test_classification_net(model, cifar10_test_loader, device)\n",
    "    accs.append(accuracy)\n",
    "\n",
    "accs = torch.tensor(accs)\n",
    "print (torch.mean(accs))\n",
    "print (torch.std(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f4a6d5b0-6426-4c36-95f0-c9c03289549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = VisionTransformer(config_vit_b_16, args.img_size, zero_head=True, num_classes=10, vis=True).cuda()\n",
    "# model.load_state_dict(torch.load('/data/home/mukhotij/ViT/Code/output/cifar10-100_500_checkpoint.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "95a99b1b-8c42-40c7-81a1-a719cdf45a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf_matrix, acc, _, _, _ = test_classification_net(model, cifar10_test_loader, device)\n",
    "\n",
    "# print (conf_matrix)\n",
    "# print (acc)\n",
    "\n",
    "# # for data, label in cifar10_test_loader:\n",
    "# #     print (data.shape)\n",
    "# #     print (data.min())\n",
    "# #     print (data.max())\n",
    "# #     print (label.shape)\n",
    "# #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "796ac5ba-51c6-4df6-ab3e-9ac0263eb4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# from torch import nn\n",
    "# from torch.nn import functional as F\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.rcParams.update({\"font.size\": 20})\n",
    "\n",
    "\n",
    "# # Some keys used for the following dictionaries\n",
    "# COUNT = \"count\"\n",
    "# CONF = \"conf\"\n",
    "# ACC = \"acc\"\n",
    "# BIN_ACC = \"bin_acc\"\n",
    "# BIN_CONF = \"bin_conf\"\n",
    "\n",
    "\n",
    "# def _bin_initializer(num_bins=10):\n",
    "#     bin_dict = {}\n",
    "#     for i in range(num_bins):\n",
    "#         bin_dict[i] = {}\n",
    "#         bin_dict[i][COUNT] = 0\n",
    "#         bin_dict[i][CONF] = 0\n",
    "#         bin_dict[i][ACC] = 0\n",
    "#         bin_dict[i][BIN_ACC] = 0\n",
    "#         bin_dict[i][BIN_CONF] = 0\n",
    "\n",
    "#     return bin_dict\n",
    "\n",
    "\n",
    "# def _populate_bins(confs, preds, labels, num_bins=10):\n",
    "\n",
    "#     bin_dict = _bin_initializer(num_bins)\n",
    "#     num_test_samples = len(confs)\n",
    "\n",
    "#     for i in range(0, num_test_samples):\n",
    "#         confidence = confs[i]\n",
    "#         prediction = preds[i]\n",
    "#         label = labels[i]\n",
    "#         binn = int(math.ceil(((num_bins * confidence) - 1)))\n",
    "#         bin_dict[binn][COUNT] = bin_dict[binn][COUNT] + 1\n",
    "#         bin_dict[binn][CONF] = bin_dict[binn][CONF] + confidence\n",
    "#         bin_dict[binn][ACC] = bin_dict[binn][ACC] + (1 if (label == prediction) else 0)\n",
    "\n",
    "#     for binn in range(0, num_bins):\n",
    "#         if bin_dict[binn][COUNT] == 0:\n",
    "#             bin_dict[binn][BIN_ACC] = 0\n",
    "#             bin_dict[binn][BIN_CONF] = 0\n",
    "#         else:\n",
    "#             bin_dict[binn][BIN_ACC] = float(bin_dict[binn][ACC]) / bin_dict[binn][COUNT]\n",
    "#             bin_dict[binn][BIN_CONF] = bin_dict[binn][CONF] / float(bin_dict[binn][COUNT])\n",
    "#     return bin_dict\n",
    "\n",
    "\n",
    "# def expected_calibration_error(confs, preds, labels, num_bins=10):\n",
    "#     bin_dict = _populate_bins(confs, preds, labels, num_bins)\n",
    "#     num_samples = len(labels)\n",
    "#     ece = 0\n",
    "#     for i in range(num_bins):\n",
    "#         bin_accuracy = bin_dict[i][BIN_ACC]\n",
    "#         bin_confidence = bin_dict[i][BIN_CONF]\n",
    "#         bin_count = bin_dict[i][COUNT]\n",
    "#         ece += (float(bin_count) / num_samples) * abs(bin_accuracy - bin_confidence)\n",
    "#     return ece\n",
    "\n",
    "\n",
    "# # Calibration error scores in the form of loss metrics\n",
    "# class ECELoss(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Compute ECE (Expected Calibration Error)\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, n_bins=15):\n",
    "#         super(ECELoss, self).__init__()\n",
    "#         bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
    "#         self.bin_lowers = bin_boundaries[:-1]\n",
    "#         self.bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "#     def forward(self, logits, labels):\n",
    "#         softmaxes = F.softmax(logits, dim=1)\n",
    "#         confidences, predictions = torch.max(softmaxes, 1)\n",
    "#         accuracies = predictions.eq(labels)\n",
    "\n",
    "#         ece = torch.zeros(1, device=logits.device)\n",
    "#         for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n",
    "#             # Calculated |confidence - accuracy| in each bin\n",
    "#             in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
    "#             prop_in_bin = in_bin.float().mean()\n",
    "#             if prop_in_bin.item() > 0:\n",
    "#                 accuracy_in_bin = accuracies[in_bin].float().mean()\n",
    "#                 avg_confidence_in_bin = confidences[in_bin].mean()\n",
    "#                 ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "\n",
    "#         return ece\n",
    "\n",
    "\n",
    "# # Methods for plotting reliability diagrams and bin-strength plots\n",
    "# def reliability_plot(confs, preds, labels, num_bins=15, model_name='model'):\n",
    "#     \"\"\"\n",
    "#     Method to draw a reliability plot from a model's predictions and confidences.\n",
    "#     \"\"\"\n",
    "#     bin_dict = _populate_bins(confs, preds, labels, num_bins)\n",
    "#     bns = [(i / float(num_bins)) for i in range(num_bins)]\n",
    "#     y = []\n",
    "#     for i in range(num_bins):\n",
    "#         y.append(bin_dict[i][BIN_ACC])\n",
    "#     plt.figure(figsize=(10, 8))  # width:20, height:3\n",
    "#     plt.bar(bns, bns, align=\"edge\", width=0.03, color=\"pink\", label=\"Expected\")\n",
    "#     plt.bar(bns, y, align=\"edge\", width=0.03, color=\"blue\", alpha=0.5, label=\"Actual\")\n",
    "#     plt.ylabel(\"Accuracy\", fontsize=30)\n",
    "#     plt.xlabel(\"Confidence\", fontsize=30)\n",
    "#     plt.xticks(fontsize=30)\n",
    "#     plt.yticks(fontsize=30)\n",
    "#     plt.legend(fontsize=30, loc='upper left')\n",
    "#     plt.savefig(f'./reliability_plot_{model_name}.pdf')\n",
    "#     plt.savefig(f'./reliability_plot_{model_name}.png')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c811ff22-17be-45b1-b5ad-7ef9a8c4524c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ece = expected_calibration_error(confidences, predictions, labels, num_bins=15)\n",
    "# print (ece)\n",
    "# reliability_plot(confidences, predictions, labels, num_bins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ff70f7a8-d0bb-41cb-a30b-025daddd36ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def entropy(logits):\n",
    "    p = F.softmax(logits, dim=1)\n",
    "    logp = F.log_softmax(logits, dim=1)\n",
    "    plogp = p * logp\n",
    "    entropy = -torch.sum(plogp, dim=1)\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def logsumexp(logits):\n",
    "    return torch.logsumexp(logits, dim=1, keepdim=False)\n",
    "\n",
    "\n",
    "def confidence(logits):\n",
    "    p = F.softmax(logits, dim=1)\n",
    "    confidence, _ = torch.max(p, dim=1)\n",
    "    return confidence\n",
    "\n",
    "\n",
    "def entropy_prob(probs):\n",
    "    p = probs\n",
    "    eps = 1e-12\n",
    "    logp = torch.log(p + eps)\n",
    "    plogp = p * logp\n",
    "    entropy = -torch.sum(plogp, dim=1)\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def mutual_information_prob(probs):\n",
    "    mean_output = torch.mean(probs, dim=0)\n",
    "    predictive_entropy = entropy_prob(mean_output)\n",
    "\n",
    "    # Computing expectation of entropies\n",
    "    p = probs\n",
    "    eps = 1e-12\n",
    "    logp = torch.log(p + eps)\n",
    "    plogp = p * logp\n",
    "    exp_entropies = torch.mean(-torch.sum(plogp, dim=2), dim=0)\n",
    "\n",
    "    # Computing mutual information\n",
    "    mi = predictive_entropy - exp_entropies\n",
    "    return mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "df9cf386-da47-4f52-82f7-e919398aac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def get_roc_auc(net, test_loader, ood_test_loader, uncertainty, device, confidence=False):\n",
    "    logits, _ = get_logits_labels(net, test_loader, device)\n",
    "    ood_logits, _ = get_logits_labels(net, ood_test_loader, device)\n",
    "\n",
    "    return get_roc_auc_logits(logits, ood_logits, uncertainty, device, confidence=confidence)\n",
    "\n",
    "\n",
    "def get_roc_auc_logits(logits, ood_logits, uncertainty, device, confidence=False):\n",
    "    uncertainties = uncertainty(logits)\n",
    "    ood_uncertainties = uncertainty(ood_logits)\n",
    "\n",
    "    # In-distribution\n",
    "    bin_labels = torch.zeros(uncertainties.shape[0]).to(device)\n",
    "    in_scores = uncertainties\n",
    "\n",
    "    # OOD\n",
    "    bin_labels = torch.cat((bin_labels, torch.ones(ood_uncertainties.shape[0]).to(device)))\n",
    "\n",
    "    if confidence:\n",
    "        bin_labels = 1 - bin_labels\n",
    "    ood_scores = ood_uncertainties  # entropy(ood_logits)\n",
    "    scores = torch.cat((in_scores, ood_scores))\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(bin_labels.cpu().numpy(), scores.cpu().numpy())\n",
    "    precision, recall, prc_thresholds = metrics.precision_recall_curve(bin_labels.cpu().numpy(), scores.cpu().numpy())\n",
    "    auroc = metrics.roc_auc_score(bin_labels.cpu().numpy(), scores.cpu().numpy())\n",
    "    auprc = metrics.average_precision_score(bin_labels.cpu().numpy(), scores.cpu().numpy())\n",
    "\n",
    "    return (fpr, tpr, thresholds), (precision, recall, prc_thresholds), auroc, auprc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e428ebac-38c2-4bcb-aa0d-9403dfcbb149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 0.9978352553267772+-0.00020354844715152136\n",
      "AUPRC: 0.9991356971197624+-0.00020354844715152136\n"
     ]
    }
   ],
   "source": [
    "aurocs = []\n",
    "auprcs = []\n",
    "for model in models:\n",
    "    (_, _, _), (_, _, _), auroc, auprc = get_roc_auc(model, cifar10_test_loader, svhn_test_loader, entropy, device)\n",
    "    aurocs.append(auroc)\n",
    "    auprcs.append(auprc)\n",
    "\n",
    "aurocs = torch.tensor(aurocs)\n",
    "auprcs = torch.tensor(auprcs)\n",
    "\n",
    "auroc_mean = torch.mean(aurocs)\n",
    "auroc_std = torch.std(aurocs)\n",
    "\n",
    "auprc_mean = torch.mean(auprcs)\n",
    "auprc_std = torch.std(auprcs)\n",
    "\n",
    "print (f\"AUROC: {auroc_mean}+-{auroc_std}\")\n",
    "print (f\"AUPRC: {auprc_mean}+-{auroc_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3615630e-bbdc-4097-b89b-c43154fa92ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 0.9844781083333335+-0.0004933044593689182\n",
      "AUPRC: 0.985478343799175+-0.0004933044593689182\n"
     ]
    }
   ],
   "source": [
    "aurocs = []\n",
    "auprcs = []\n",
    "for model in models:\n",
    "    (_, _, _), (_, _, _), auroc, auprc = get_roc_auc(model, cifar10_test_loader, cifar100_test_loader, entropy, device)\n",
    "    aurocs.append(auroc)\n",
    "    auprcs.append(auprc)\n",
    "\n",
    "aurocs = torch.tensor(aurocs)\n",
    "auprcs = torch.tensor(auprcs)\n",
    "\n",
    "auroc_mean = torch.mean(aurocs)\n",
    "auroc_std = torch.std(aurocs)\n",
    "\n",
    "auprc_mean = torch.mean(auprcs)\n",
    "auprc_std = torch.std(auprcs)\n",
    "\n",
    "print (f\"AUROC: {auroc_mean}+-{auroc_std}\")\n",
    "print (f\"AUPRC: {auprc_mean}+-{auroc_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "335e9bb3-95df-45e2-9ac5-8f9274139310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 0.996324323909035+-0.00037792024791639695\n",
      "AUPRC: 0.9932660611948467+-0.00037792024791639695\n"
     ]
    }
   ],
   "source": [
    "aurocs = []\n",
    "auprcs = []\n",
    "for model in models:\n",
    "    (_, _, _), (_, _, _), auroc, auprc = get_roc_auc(model, cifar10_test_loader, svhn_test_loader, confidence, device, confidence=True)\n",
    "    aurocs.append(auroc)\n",
    "    auprcs.append(auprc)\n",
    "\n",
    "aurocs = torch.tensor(aurocs)\n",
    "auprcs = torch.tensor(auprcs)\n",
    "\n",
    "auroc_mean = torch.mean(aurocs)\n",
    "auroc_std = torch.std(aurocs)\n",
    "\n",
    "auprc_mean = torch.mean(auprcs)\n",
    "auprc_std = torch.std(auprcs)\n",
    "\n",
    "print (f\"AUROC: {auroc_mean}+-{auroc_std}\")\n",
    "print (f\"AUPRC: {auprc_mean}+-{auroc_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3ff3c52b-fc3b-4e66-8142-f102b03f5179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 0.9829110766666668+-0.0005265952466633619\n",
      "AUPRC: 0.9825370548250124+-0.0005265952466633619\n"
     ]
    }
   ],
   "source": [
    "aurocs = []\n",
    "auprcs = []\n",
    "for model in models:\n",
    "    (_, _, _), (_, _, _), auroc, auprc = get_roc_auc(model, cifar10_test_loader, cifar100_test_loader, confidence, device, confidence=True)\n",
    "    aurocs.append(auroc)\n",
    "    auprcs.append(auprc)\n",
    "\n",
    "aurocs = torch.tensor(aurocs)\n",
    "auprcs = torch.tensor(auprcs)\n",
    "\n",
    "auroc_mean = torch.mean(aurocs)\n",
    "auroc_std = torch.std(aurocs)\n",
    "\n",
    "auprc_mean = torch.mean(auprcs)\n",
    "auprc_std = torch.std(auprcs)\n",
    "\n",
    "print (f\"AUROC: {auroc_mean}+-{auroc_std}\")\n",
    "print (f\"AUPRC: {auprc_mean}+-{auroc_std}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
